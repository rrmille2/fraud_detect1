{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fraud Detector \n",
    "### Supervised fraud detection using external (S3) data\n",
    "-------\n",
    "- [Introduction](#Introduction)\n",
    "- [Setup](#Setup)\n",
    "- [Plan](#Plan)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "-------\n",
    "\n",
    "Amazon Fraud Detector (AFD) is a fully managed service that makes it easy to identify potentially fraudulent online activities such as online payment fraud and the creation of fake accounts. Fraud Detector capitalizes on the latest advances in machine learning (ML) and 20 years of fraud detection expertise from AWS and Amazon.com to automatically identify potentially fraudulent activity so you can catch more fraud faster.\n",
    "\n",
    "In this notebook, we'll use the Amazon Fraud Detector API to define an entity and an event of interest and use CSV data stored in S3 to train a model. Next, we'll derive some rules and create a \"detector\" by combining our entity, event, model, and rules into a single endpoint. Finally, we'll apply the detector to a sample of our data to identify potentially fraudulent events.\n",
    "\n",
    "After running this notebook you should be able to:\n",
    "- Define an Entity and an Event\n",
    "- Create a Detector\n",
    "- Train a Machine Learning (ML) Model\n",
    "- Author Rules to identify potential fraud based on the model's score\n",
    "- Apply the Detector's \"predict\" function, to generate a model score and rule outcomes on data\n",
    "\n",
    "**Notes**:  \n",
    "\n",
    "Currently, only Online Fraud Insights template supports building model with external events.\n",
    "\n",
    "```python\n",
    "    trainingDataSource  = 'EXTERNAL_EVENTS',\n",
    "    trainingDataSchema  = training_data_schema,\n",
    "    externalEventsDetail = {\n",
    "        'dataLocation'     : S3_FILE_LOC,\n",
    "        'dataAccessRoleArn': ARN_ROLE\n",
    "    }\n",
    "```\n",
    "\n",
    "If you would like to know more, please check out [Fraud Detector's Documentation](https://docs.aws.amazon.com/frauddetector/). \n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "------\n",
    "First setup your AWS credentials so that Amazon Fraud Detector can store and access training data and supporting detector artifacts in S3.\n",
    "\n",
    "\n",
    "### Setting up AWS Credentials & Permissions\n",
    "\n",
    "https://docs.aws.amazon.com/frauddetector/latest/ug/set-up.html\n",
    "\n",
    "To use Amazon Fraud Detector, you have to set up permissions that allow access to the Amazon Fraud Detector console and API operations. You also have to allow Amazon Fraud Detector to perform tasks on your behalf and to access resources that you own. We recommend creating an AWS Identify and Access Management (IAM) user with access restricted to Amazon Fraud Detector operations and required permissions. You can add other permissions as needed.\n",
    "\n",
    "The following policies provide the required permission to use Amazon Fraud Detector. If you are using SageMaker Notebook Instance, add the following two policies to the Instance's IAM role and restart your kernel:\n",
    "\n",
    "- *AmazonFraudDetectorFullAccessPolicy*  \n",
    "    Allows you to perform the following actions:  \n",
    "        - Access all Amazon Fraud Detector resources  \n",
    "        - List and describe all model endpoints in Amazon SageMaker  \n",
    "        - List all IAM roles in the account  \n",
    "        - List all Amazon S3 buckets  \n",
    "        - Allow IAM Pass Role to pass a role to Amazon Fraud Detector  \n",
    "\n",
    "- *AmazonS3FullAccess*  \n",
    "    Allows full access to Amazon S3. This is required to upload training files to S3.\n",
    "    \n",
    "    \n",
    "    \n",
    "## Plan\n",
    "------\n",
    "A *Detector* contains the event, model(s) and rule(s) detection logic for a particular type of fraud that you want to detect. We'll use the following 7 step process to plan a Fraud Detector: \n",
    "\n",
    " \n",
    "1. [Setup notebook](#setup_notebook)<br>\n",
    "    a. Name the major components: entity, entity type, model, detector <br>\n",
    "    b. Plug in your ARN role <br>\n",
    "    c. Plug in your S3 Bucket and CSV File\n",
    "    \n",
    "2. [Load and profile your dataset](#load_and_profile_your_data)<br>\n",
    "    a. This will give you an idea of what your dataset contains<br>\n",
    "    b. This will also identify the variables and labels that will need to be created to define your event\n",
    "\n",
    "3. [Create event variables and labels](#create_event_variables_and_labels)<br>\n",
    "    a. This will create the variables and labels in fraud detector \n",
    "\n",
    "4. [Define your Entity and Event Type](#define_your_entity_and_event_type)<br>\n",
    "    a. What is activity that you are detecting? That's likely your Event Type (e.g. transaction)<br>\n",
    "    b. Who is performing this activity? That's likely your Entity (e.g. customer)\n",
    "     \n",
    "5. [Create and train your model](#create_and_train_your_model)\t\n",
    "    a. Model training takes anywhere from 45-60 minutes. Once complete you need to promote your endpoint<br>\n",
    "    b. Promote your model\n",
    "\n",
    "6. [Create a Fraud Detector, generate Rules and assemble your Detector](#create_detector)<br>\n",
    "    a. Create your detector<br>\n",
    "    b. Define outcomes, e.g. fraud, investigate and approve<br>\n",
    "    c. Create rules based on your model scores <br>\n",
    "    d. Assemble your detector: combine your rule(s) and model into a \"detector\"\n",
    "\n",
    "7. [Make predictions](#make_predictions)<br>\n",
    "    a. Interactively call predict API on a handful of records   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- import packages --\n",
    "import boto3\n",
    "import time\n",
    "import logging\n",
    "import uuid \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, roc_auc_score\n",
    "\n",
    "# -- for display --\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:90% }</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "%matplotlib inline \n",
    "\n",
    "# original_boto3_version = boto3.__version__\n",
    "# original_boto3_version\n",
    "# %pip install 'boto3>1.17.21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -- initialize the AFD client \n",
    "client = boto3.client('frauddetector') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup notebook <a id='setup_notebook'></a>\n",
    "-----\n",
    "\n",
    "***To get started***\n",
    "\n",
    "1. Name the major components of Fraud Detector\n",
    "2. Specify the MODEL_TYPE; currently only ONLINE_FRAUD_INSIGHTS support external data source\n",
    "2. Plug in your ARN role \n",
    "3. Plug in your S3 Bucket and CSV File \n",
    "\n",
    "Then you can interactively exeucte the code cells in the notebook, no need to change anything unless you want to. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Fraud Detector Components </strong>\n",
    "    \n",
    "- **EVENT_TYPE** is a business activity that you want evaluated for fraud risk \n",
    "- **ENTITY_TYPE** represents the \"what or who\" that is performing the event you want to evaluate\n",
    "- **MODEL_NAME** is the name of your supervised machine learning model that Fraud Detector trains on your behalf\n",
    "- **DETECTOR_NAME** is the name of the detector that contains the detection logic (model and rules) that you apply to events that you want to evaluate for fraud\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Identify the following assets:\n",
    "<div class=\"alert alert-info\"><strong>  Bucket, File, and ARN Role </strong>\n",
    "\n",
    "- **ARN_ROLE** is the role Fraud Detector use to access your data in s3 bucket\n",
    "- **S3_BUCKET** is the name of the bucket where your file lives\n",
    "- **S3_FILE_TRAIN** is the URL to your s3 training csv file\n",
    "- **S3_FILE_TEST** is the URL to your s3 testing csv file\n",
    "\n",
    "</div>\n",
    "\n",
    "_**Note**: To use Amazon Fraud Detector, you have to set up permissions that allow access to the Amazon Fraud Detector console and API operations. You also have to allow Amazon Fraud Detector to perform tasks on your behalf and to access resources that you own. We recommend creating an AWS Identify and Access Management (IAM) user with access restricted to. Amazon Fraud Detector operations and required permissions. You can add other permissions as needed. See \"Create an IAM User and Assign Required Permissions\" in the user's guide: https://docs.aws.amazon.com/frauddetector/latest/ug/set-up.html_\n",
    "\n",
    "_**Note**: For OFI template with external data, you must have_ `EVENT_TIMESTAMP` _and_ `EVENT_LABEL` _columns with exactly the names in your data file._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- this is all you need to fill out, once complete simply interactively run each code cell --  \n",
    "EVENT_TYPE     = \"claim-event\"\n",
    "EVENT_DESC     = \"Look for fraudulent insurance claims\"\n",
    "\n",
    "ENTITY_TYPE    = \"policy_holder\"\n",
    "ENTITY_DESC    = \"Policy Holder\"\n",
    "\n",
    "MODEL_NAME     = \"fraud_model\"\n",
    "MODEL_DESC     = \"Model to detect fraudulent claims\"\n",
    "\n",
    "DETECTOR_NAME  = \"fraud_detector\"                       \n",
    "DETECTOR_DESC  = \"Claim fraud detector model and rules\"\n",
    "\n",
    "MODEL_TYPE     = \"ONLINE_FRAUD_INSIGHTS\"                  # currently only ONLINE_FRAUD_INSIGHTS support external data source\n",
    "\n",
    "ARN_ROLE       = \"your-arn-role\"                        \n",
    "S3_BUCKET      = \"your-bucket-name\"              \n",
    "S3_FILE_TRAIN  = \"your-train-csv-file\"        \n",
    "S3_FILE_TEST   = \"your-test-csv-file\"\n",
    "S3_FILE_TRAIN_LOC = \"s3://{0}/{1}\".format(S3_BUCKET,S3_FILE_TRAIN)\n",
    "\n",
    "\n",
    "# -- config of your data --\n",
    "# 1. for OFI template with external data, you must have EVENT_TIMESTAMP, EVENT_LABEL columns in your data\n",
    "# 2. map other variables to the column names in your data, options include: AUTH_CODE, AVS, BILLING_ADDRESS_L1, \n",
    "#      BILLING_ADDRESS_L2, BILLING_CITY, BILLING_COUNTRY, BILLING_NAME, BILLING_PHONE, BILLING_STATE, BILLING_ZIP, \n",
    "#      CARD_BIN CATEGORICAL, CURRENCY_CODE, EMAIL_ADDRESS, FINGERPRINT, FREE_FORM_TEXT, IP_ADDRESS, NUMERIC, PAYMENT_TYPE, \n",
    "#      PHONE_NUMBER, PRICE, PRODUCT_CATEGORY, SHIPPING_ADDRESS_L1, SHIPPING_ADDRESS_L2, \n",
    "#      SHIPPING_CITY, SHIPPING_COUNTRY, SHIPPING_NAME, SHIPPING_PHONE, SHIPPING_STATE, SHIPPING_ZIP, USERAGENT\n",
    "# 3. for features not mapped in the list, the summary_stats function will guess the variable types based on variable name and data type. \n",
    "#      We recommend you map all your variables to corresponding types list in 2.\n",
    "VARIABLES_MAP = {\n",
    "    # required\n",
    "    \"EVENT_TIMESTAMP\": \"EVENT_TIMESTAMP\",  \n",
    "    \"EVENT_LABEL\": \"EVENT_LABEL\",          \n",
    "    # optional\n",
    "    #\"IP_ADDRESS\": \"column-name-of-ip-address\",            # e.g. ip_address\n",
    "    #\"EMAIL_ADDRESS\": \"column-name-of-email\",              # e.g. customer_email\n",
    "    \"NUMERIC\": [\n",
    "        'months_as_customer',\n",
    "        'age',\n",
    "        'policy_deductable',\n",
    "        'policy_annual_premium',\n",
    "        'umbrella_limit',\n",
    "        'incident_hour_of_the_day',\n",
    "        'number_of_vehicles_involved',\n",
    "        'bodily_injuries',\n",
    "        'witnesses'        \n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and profile your dataset <a id='load_and_profile_your_data'></a>\n",
    "-----\n",
    "\n",
    "The functions below will: 1) profile your data, creating descriptive statististics, 2) perform basic data quality checks (nulls, unique variables, etc.), and 3) return summary statistics and the EVENT and MODEL schemas used to define your EVENT_TYPE and train your model. \n",
    "\n",
    "\n",
    "_**Important Note**: The functions below provides a layman guess for the fraud/legit labels and variable mapping. Please review the summary stats, event variables, event labels and training data schema and make sure they are aligned with how you want to use the data. You can always manually modify them if needed._\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> summary stats, event variables, event labels and training data schema </strong>\n",
    "\n",
    "- summary stats: data quality and summary statistics of the data; used to create variables of the specific feature types\n",
    "- event variables: variables associated with the specific event type; used when creating event type\n",
    "- event labels: labels associated with the event type; used when creating event type\n",
    "- training data schema: define the variables to build the model, labels to be used as fraud/legit, and how to treat the unlabeled events; By default, we identify the rare event as fraud, and the rest as not-fraud. If you have more than 2 labels in your data or want to map them in a different way, you can manually modify the training data schema \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- no changes; just run this code block ---\n",
    "def guess_type_based_on_name(name):\n",
    "    \"\"\"\n",
    "    Guess feature type based on its name. This is a help function used in summary_stats\n",
    "    \"\"\"\n",
    "    name = name.replace(\"_\",\"\").lower()\n",
    "    guess_type = []\n",
    "    guess_map = {\n",
    "        'IP_ADDRESS': ['ipaddr'],\n",
    "        'EMAIL_ADDRESS': ['email'],\n",
    "        'CARD_BIN': ['cardbin','cardnum'],\n",
    "        'PHONE_NUMBER': ['phone'],\n",
    "        'USERAGENT': ['useragent', 'ua'],\n",
    "        'PRICE': ['price'],\n",
    "        'PRODUCT_CATEGORY':['productcategory','prodcategory'],\n",
    "        'CURRENCY_CODE': ['currency'],\n",
    "        'BILLING_ADDRESS_L1': ['billingstreet','billstreet'],\n",
    "        'BILLING_CITY': ['billingcity','billcity'],\n",
    "        'BILLING_STATE': ['billingstate','billstate'],\n",
    "        'BILLING_ZIP': ['billingzip','billzip','billingpostal','billpostal'],\n",
    "        'EVENT_ID': ['eventid'],\n",
    "        'ENTITY_ID': ['entityid', 'customerid'],\n",
    "        'LABEL_TIMESTAMP': ['labeltimestamp'],\n",
    "        'ENTITY_TYPE': ['entity_type']\n",
    "    }\n",
    "    \n",
    "    for var in guess_map:\n",
    "        if any(x in name for x in guess_map[var]):\n",
    "            guess_type.append(var) \n",
    "            \n",
    "    return guess_type\n",
    "\n",
    "\n",
    "def summary_stats(df, variables_map):\n",
    "    \"\"\"\n",
    "    Generate summary statistics for a pandas data frame  \n",
    "    \"\"\"\n",
    "    rowcnt = len(df)\n",
    "    \n",
    "    # -- calculating data statistics and data types -- \n",
    "    df_s1  = df.agg(['count', 'nunique']).transpose().reset_index().rename(columns={\"index\":\"feature_name\"})\n",
    "    df_s1[\"null\"] = (rowcnt - df_s1[\"count\"]).astype('int64')\n",
    "    df_s1[\"not_null\"] = rowcnt - df_s1[\"null\"]\n",
    "    df_s1[\"null_pct\"] = df_s1[\"null\"] / rowcnt\n",
    "    df_s1[\"nunique_pct\"] = df_s1['nunique']/ rowcnt\n",
    "\n",
    "    dt = pd.DataFrame(df.dtypes).reset_index().rename(columns={\"index\":\"feature_name\", 0:\"dtype\"})\n",
    "    df_stats = pd.merge(dt, df_s1, on='feature_name', how='inner').round(4)\n",
    "    df_stats['nunique'] = df_stats['nunique'].astype('int64')\n",
    "    df_stats['count'] = df_stats['count'].astype('int64')\n",
    "    \n",
    "    # -- variable type mapper: map variables configured in variables_map -- \n",
    "    flatten_var_maps = []\n",
    "    for vartype in variables_map.keys():\n",
    "        if isinstance(variables_map[vartype], list):\n",
    "            for var in variables_map[vartype]:\n",
    "                flatten_var_maps.append([vartype, var])\n",
    "        else:\n",
    "            flatten_var_maps.append([vartype, variables_map[vartype]])\n",
    "    df_schema = pd.DataFrame(flatten_var_maps, columns = ['feature_type', 'feature_name'])\n",
    "    df_stats = pd.merge(df_stats, df_schema, how = 'left', on = 'feature_name')\n",
    "    \n",
    "    # -- variable type mapper: guess types based on feature names -- \n",
    "    guess_types = df_stats.loc[df_stats['feature_type'].isna(),'feature_name'].apply(guess_type_based_on_name)\n",
    "    df_stats.loc[df_stats['feature_type'].isna(),'feature_type'] = guess_types[guess_types.apply(len) == 1].apply(lambda x: x[0])\n",
    "    \n",
    "    # -- variable type mapper: map the rest types based on data type -- \n",
    "    df_stats.loc[(df_stats['feature_type'].isna())&(df_stats[\"dtype\"] == object), 'feature_type'] = \"CATEGORICAL\"\n",
    "    df_stats.loc[(df_stats['feature_type'].isna())&((df_stats[\"dtype\"] == \"int64\") | (df_stats[\"dtype\"] == \"float64\")), 'feature_type'] = \"NUMERIC\"\n",
    "    \n",
    "    # -- variable validation -- \n",
    "    df_stats['feature_warning'] = \"NO WARNING\"\n",
    "    df_stats.loc[(df_stats[\"nunique\"] != 2) & (df_stats[\"feature_name\"] == \"EVENT_LABEL\"),'feature_warning' ] = \"LABEL WARNING, NON-BINARY EVENT LABEL\"\n",
    "    df_stats.loc[(df_stats[\"nunique_pct\"] > 0.9) & (df_stats['feature_type'] == \"CATEGORICAL\") ,'feature_warning' ] = \"EXCLUDE, GT 90% UNIQUE\"\n",
    "    df_stats.loc[(df_stats[\"null_pct\"] > 0.2) & (df_stats[\"null_pct\"] <= 0.75), 'feature_warning' ] = \"NULL WARNING, GT 20% MISSING\"\n",
    "    df_stats.loc[df_stats[\"null_pct\"] > 0.75,'feature_warning' ] = \"EXCLUDE, GT 75% MISSING\"\n",
    "    df_stats.loc[((df_stats['dtype'] == \"int64\" ) | (df_stats['dtype'] == \"float64\" ) ) & (df_stats['nunique'] < 0.2), 'feature_warning' ] = \"LIKELY CATEGORICAL, NUMERIC w. LOW CARDINALITY\"\n",
    "    return df_stats\n",
    "\n",
    "\n",
    "def prepare_schema(df, df_stats, variables_map):\n",
    "    \"\"\"\n",
    "    Prepare schema for following steps\n",
    "    \"\"\"\n",
    "    # -- event variables --\n",
    "    exclude_list = ['EVENT_LABEL','EVENT_TIMESTAMP','ENTITY_ID','EVENT_ID','LABEL_TIMESTAMP','ENTITY_TYPE','UNKNOWN']\n",
    "    event_variables = df_stats.loc[(~df_stats['feature_type'].isin(exclude_list))]['feature_name'].to_list()\n",
    "    \n",
    "    # -- target -- \n",
    "    label_value_count = df[variables_map['EVENT_LABEL']].dropna().astype('str', errors='ignore').value_counts()\n",
    "    event_labels    = label_value_count.index.unique().tolist()  \n",
    "    \n",
    "    # -- define training_data_schema --\n",
    "    training_data_schema = {\n",
    "        'modelVariables' : df_stats.loc[~(df_stats['feature_type'].isin(exclude_list))]['feature_name'].to_list(),\n",
    "        'labelSchema'    : {\n",
    "            # we assume the rare event as fraud, and the rest as not-fraud. \n",
    "            # if you have more than 2 labels in the data or want to map them in a different way, you can manually modify the training data schema\n",
    "            'labelMapper' : {\n",
    "                'FRAUD' : [str(label_value_count.idxmin())],\n",
    "                'LEGIT' : [i for i in event_labels if i not in [str(label_value_count.idxmin())]]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return training_data_schema, event_variables, event_labels\n",
    "\n",
    "\n",
    "def profiling(df, variables_map):\n",
    "    \"\"\"\n",
    "    profiling the input pandas data frame and prepare schema for following steps  \n",
    "    \n",
    "    Arguments:\n",
    "        df (DataFrame)             - panda's dataframe to create summary statistics for\n",
    "        variables_map (dictionary) - variables map dictionary - key is the variable type and value is the list of variable name\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame of summary statistics, training data schema, event variables and event labels  \n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # -- check required variables --\n",
    "    missing_required_vars = [i for i in ['EVENT_LABEL', 'EVENT_TIMESTAMP'] if i not in set(variables_map.keys())]\n",
    "    if len(missing_required_vars) != 0:\n",
    "        logging.error(\"Required variables {0} are not mapped.\".format(missing_required_vars))\n",
    "    \n",
    "    # -- get data summary --\n",
    "    df_stats = summary_stats(df, variables_map)\n",
    "    \n",
    "    # -- prepare schema for following steps -- \n",
    "    training_data_schema, event_variables, event_labels = prepare_schema(df, df_stats, variables_map)\n",
    "    \n",
    "    print(\"--- summary stats ---\")\n",
    "    print(df_stats)\n",
    "    print(\"\\n\")\n",
    "    print(\"--- event variables ---\")\n",
    "    print(event_variables)\n",
    "    print(\"\\n\")\n",
    "    print(\"--- event labels ---\")\n",
    "    print(event_labels)\n",
    "    print(\"\\n\")\n",
    "    print(\"--- training data schema ---\")\n",
    "    print(training_data_schema)\n",
    "    \n",
    "    return df_stats, training_data_schema, event_variables, event_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- connect to S3, snag file, and convert to a panda's dataframe --\n",
    "s3   = boto3.resource('s3')\n",
    "obj  = s3.Object(S3_BUCKET, S3_FILE_TRAIN)\n",
    "body = obj.get()['Body']\n",
    "df   = pd.read_csv(body, dtype={VARIABLES_MAP['EVENT_LABEL']: object})\n",
    "    \n",
    "# -- call profiling function -- \n",
    "df_stats, training_data_schema, event_variables, event_labels = profiling(df, VARIABLES_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create event variables and labels <a id='create_event_variables_and_labels'></a> \n",
    "-----\n",
    "\n",
    "The following section will automatically create your modeling input variables for you. \n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> APIs for Creating/Deleting Variables and Labels </strong>\n",
    "  \n",
    "- **create_variable**: Creates a variable in Fraud Detector\n",
    "- **get_variables**: Gets all of the variables or a specific label if name is provided\n",
    "- **delete_variables**: Delete a variable; If you have events, models or detectors created using the variable, you need to delect the associated resource first\n",
    "- **put_label**: Creates a label\n",
    "- **get_labels**: Gets all labels or a specific label if name is provided\n",
    "- **delete_label**: Delete a label \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -- function to create all your variables --- \n",
    "def create_variables(features_dict):\n",
    "    \"\"\"\n",
    "    Check if variables exist, if not, adds the variable to Fraud Detector \n",
    "    \n",
    "    Arguments: \n",
    "        features_dict  -  a dictionary maps your variables to variable type\n",
    "    \"\"\"\n",
    "    for feature in features_dict.keys(): \n",
    "        DEFAULT_VALUE = '0.0' if features_dict[feature] in ['NUMERIC','PRICE'] else '<null>'\n",
    "        DATA_TYPE = 'FLOAT' if features_dict[feature] in ['NUMERIC','PRICE'] else 'STRING'\n",
    "        \n",
    "        try:\n",
    "            resp = client.get_variables(name = feature)\n",
    "            features_dict[feature] = resp['variables'][0]['dataType']\n",
    "            print(\"{0} exists, data type: {1}\".format(feature, features_dict[feature]))\n",
    "        except:\n",
    "            print(\"Creating variable: {0}\".format(feature))\n",
    "            resp = client.create_variable(\n",
    "                    name         = feature,\n",
    "                    dataType     = DATA_TYPE,\n",
    "                    dataSource   ='EVENT',\n",
    "                    defaultValue = DEFAULT_VALUE, \n",
    "                    description  = feature,\n",
    "                    variableType = features_dict[feature])\n",
    "    return features_dict\n",
    "\n",
    "\n",
    "# -- function to create all your labels --- \n",
    "def create_label(label_mapper):\n",
    "    \"\"\"\n",
    "    Add labels to Fraud Detector\n",
    "    \n",
    "    Arguments:\n",
    "        label_mapper   - a dictionary maps Fraud/Legit to your labels in data\n",
    "    \"\"\"\n",
    "    for label in label_mapper['FRAUD']:\n",
    "        response = client.put_label(\n",
    "            name = label,\n",
    "            description = \"FRAUD\")\n",
    "    \n",
    "    for label in label_mapper['LEGIT']:\n",
    "        response = client.put_label(\n",
    "            name = label,\n",
    "            description = \"LEGIT\")\n",
    "\n",
    "\n",
    "exclude_list = ['EVENT_ID', 'ENTITY_ID','EVENT_TIMESTAMP','EVENT_LABEL','LABEL_TIMESTAMP','ENTITY_TYPE','UNKNOWN']\n",
    "features_dict = df_stats.loc[(~df_stats['feature_type'].isin(exclude_list))].set_index('feature_name')['feature_type'].to_dict()\n",
    "print(\"\\n --- model variable dict --\")\n",
    "features_dict = create_variables(features_dict)\n",
    "print(\"\\n\")\n",
    "print(features_dict)\n",
    "\n",
    "label_mapper = training_data_schema['labelSchema']['labelMapper']\n",
    "print(\"\\n --- model label schema dict --\")\n",
    "print(label_mapper)\n",
    "create_label(label_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define your Entity and Event Types <a id='define_your_entity_and_event_type'></a>\n",
    "-----\n",
    "  \n",
    "The following code block will automatically create your entity and event types for you.\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> APIs for Entity and Event Types </strong>\n",
    "\n",
    "- **put_entity_type**: Creates or updates an entity type. An entity represents who is performing the event. An entity type classifies the entity. Example classifications include customer, merchant, or account\n",
    "- **get_entity_type**: Gets all entity types or a specific entity type if a name is specified\n",
    "- **delete_entity_type**: Deletes an entity type. If you have an event type associated with the entity type, you need to delete that event type first \n",
    "- **put_event_type**: Creates or updates an event type. An event is a business activity that is evaluated for fraud risk. Example event types include online payment transactions, account registrations, and authentications\n",
    "- **get_event_type**: Gets all event types or a specific event type if name is provided\n",
    "- **delete_event_typ**e: Delete one event type \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- no changes just run this code block ---\n",
    "# -- create entity type --\n",
    "try:\n",
    "    response = client.get_entity_types(name = ENTITY_TYPE)\n",
    "    print(\"-- entity type exists --\")\n",
    "    print(response)\n",
    "except:\n",
    "    response = client.put_entity_type(\n",
    "        name        = ENTITY_TYPE,\n",
    "        description = ENTITY_DESC\n",
    "    )\n",
    "    print(\"-- create entity type --\")\n",
    "    print(response)\n",
    "    \n",
    "\n",
    "# -- create event type --\n",
    "try:\n",
    "    response = client.get_event_types(name = EVENT_TYPE)\n",
    "    print(\"\\n-- event type exists --\")\n",
    "    print(response)\n",
    "except:\n",
    "    response = client.put_event_type (\n",
    "        name           = EVENT_TYPE,\n",
    "        eventVariables = event_variables,\n",
    "        labels         = event_labels,\n",
    "        entityTypes    = [ENTITY_TYPE])\n",
    "    print(\"\\n-- create event type --\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create and train your model <a id=create_and_train_your_model></a>\n",
    "-----\n",
    "\n",
    "The following section will automatically train and activate your model for you. By default, we use all features available, if you want to exclude features from training, you can review and modify the `training_data_schema`.\n",
    "    \n",
    "<div class=\"alert alert-info\"> 💡 <strong> APIs for Creating and Training Model </strong>\n",
    "\n",
    "- **create_model**: Creates a model using the specified model type. Available model types include: ONLINE_FRAUD_INSIGHTS, TRANSACTION_FRAUD_INSIGHTS\n",
    "- **update_model**: Updates a model. You can update the description attribute using this action\n",
    "- **get_models**: Gets one or more models. Gets all models for the AWS account if no model type and no model id provided\n",
    "- **create_model_version**: Creates a version of the model using the specified model type and model id\n",
    "- **update_model_version**: Updates a model version. Updating a model version retrains an existing model version using updated training data and produces a new minor version of the model. You can update the training data set location and data access role attributes using this action. This action creates and trains a new minor version of the model, for example version 1.01, 1.02, 1.03\n",
    "- **describe_model_versions**: Gets all of the model versions for the specified model type or for the specified model type and model ID. You can also get details for a single, specified model version\n",
    "- **get_model_version**: Gets the details of the specified model version\n",
    "- **put_external_model**: Creates or updates an Amazon SageMaker model endpoint. You can also use this action to update the configuration of the model endpoint, including the IAM role and/or the mapped variables\n",
    "- **get_external_models**: Gets the details for one or more Amazon SageMaker models that have been imported into the service\n",
    "- **update_model_version_status**: Updates the status of a model version. You can 1) Change the TRAINING_COMPLETE status to ACTIVE, 2) Change ACTIVEto INACTIVE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- no changes; just run this code block ---\n",
    "\n",
    "# -- create the model --\n",
    "try:\n",
    "    response = client.create_model(\n",
    "       description   = MODEL_DESC,\n",
    "       eventTypeName = EVENT_TYPE,\n",
    "       modelId       = MODEL_NAME,\n",
    "       modelType     = MODEL_TYPE)\n",
    "    print(\"-- initalize model --\")\n",
    "    print(response)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- no changes; just run this code block ---\n",
    "\n",
    "# -- initalizes the model, it's now ready to train --\n",
    "response = client.create_model_version(\n",
    "    modelId             = MODEL_NAME,\n",
    "    modelType           = MODEL_TYPE,\n",
    "    trainingDataSource  = 'EXTERNAL_EVENTS',\n",
    "    trainingDataSchema  = training_data_schema,\n",
    "    externalEventsDetail = {\n",
    "        'dataLocation'     : S3_FILE_TRAIN_LOC,\n",
    "        'dataAccessRoleArn': ARN_ROLE\n",
    "    }\n",
    ")\n",
    "model_version = response['modelVersionNumber']\n",
    "print(\"-- model training --\")\n",
    "print(response)\n",
    "\n",
    "# -- model training can take a long time, we'll loop until it's complete  -- \n",
    "print(\"-- wait for model training to complete --\") \n",
    "stime = time.time()\n",
    "while True:\n",
    "    clear_output(wait = True)\n",
    "    response = client.get_model_version(modelId = MODEL_NAME, modelType = MODEL_TYPE, modelVersionNumber = model_version)\n",
    "    if response['status'] == 'TRAINING_IN_PROGRESS':\n",
    "        print(f\"current progress: {(time.time() - stime)/60:{3}.{3}} minutes\")\n",
    "        time.sleep(60)  # -- sleep for 60 seconds \n",
    "    if response['status'] != 'TRAINING_IN_PROGRESS':\n",
    "        print(\"Model status : \" +  response['status'])\n",
    "        break\n",
    "etime = time.time()\n",
    "\n",
    "# -- summarize --\n",
    "print(\"\\n --- model training complete  --\")\n",
    "print(\"Elapsed time : %s\" % (etime - stime) + \" seconds \\n\"  )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- activate the model version --\n",
    "response = client.update_model_version_status (\n",
    "    modelId            = MODEL_NAME,\n",
    "    modelType          = MODEL_TYPE,\n",
    "    modelVersionNumber = model_version,\n",
    "    status             = 'ACTIVE'\n",
    ")\n",
    "print(\"-- activating model --\")\n",
    "print(response)\n",
    "\n",
    "# -- wait until model is active --\n",
    "print(\"--- waiting until model status is active \")\n",
    "stime = time.time()\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    response = client.get_model_version(modelId=MODEL_NAME, modelType = MODEL_TYPE, modelVersionNumber = model_version)\n",
    "    if response['status'] != 'ACTIVE':\n",
    "        print(f\"current progress: {(time.time() - stime)/60:{3}.{3}} minutes\")\n",
    "        time.sleep(60)  # sleep for 1 minute \n",
    "    if response['status'] == 'ACTIVE':\n",
    "        print(\"Model status : \" +  response['status'])\n",
    "        break\n",
    "        \n",
    "etime = time.time()\n",
    "print(\"Elapsed time : %s\" % (etime - stime) + \" seconds \\n\"  )\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- model performance summary -- \n",
    "trainingMetrics = client.describe_model_versions(\n",
    "    modelId            = MODEL_NAME,\n",
    "    modelVersionNumber = model_version,\n",
    "    modelType          = MODEL_TYPE,\n",
    "    maxResults         = 10\n",
    ")['modelVersionDetails'][0]['trainingResult']['trainingMetrics']\n",
    "\n",
    "perf_auc = trainingMetrics['auc']\n",
    "df_model = pd.DataFrame(trainingMetrics['metricDataPoints'])\n",
    "\n",
    "# -- ROC Chart -- \n",
    "plt.figure(figsize = (8,8))\n",
    "plt.plot(df_model[\"fpr\"], df_model[\"tpr\"], color='darkorange', lw=2, label='ROC curve (area = %0.3f)'%perf_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(MODEL_NAME+' ROC Chart')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.axvline(x=0.03, linewidth=2, color='r')\n",
    "plt.axhline(y=0.75, linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- variable importance summary -- \n",
    "varImpMetrics = client.describe_model_versions(\n",
    "    modelId            = MODEL_NAME,\n",
    "    modelVersionNumber = model_version,\n",
    "    modelType          = MODEL_TYPE,\n",
    "    maxResults         = 10\n",
    ")['modelVersionDetails'][0]['trainingResult']['variableImportanceMetrics']\n",
    "\n",
    "df_var_imp = pd.DataFrame(varImpMetrics['logOddsMetrics']).sort_values(by='variableImportance')\n",
    "\n",
    "# -- Variable importance Chart -- \n",
    "df_var_imp.plot.barh(x='variableName',y='variableImportance',figsize=(10,int(0.5*df_var_imp.shape[0])))\n",
    "plt.xlabel('Variable Importance (logOdds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a Fraud Detector, generate Rules and assemble your Detector <a id = create_detector></a>\n",
    "-----\n",
    "The following section will automatically generate a number of fraud, investigate and approve rules based on the false positive rate and score thresholds of your model. These are just example rules that you could create, it is recommended that you fine tune your rules specifically to your business use case.  \n",
    "    \n",
    "<div class=\"alert alert-info\"> 💡 <strong> Key APIs for Generating Rules, Creating and Publishing a Detector </strong>\n",
    "    \n",
    "- **put_detector**: Creates or updates a detector\n",
    "- **put_outcome**: Creates or updates an outcome\n",
    "- **create_rule**: Creates a rule for use with the specified detector\n",
    "- **update_rule_version**: Updates a rule version resulting in a new rule version (version 1, 2, 3 ...)\n",
    "- **create_detector_version**: Creates a detector version. The detector version starts in a DRAFT status\n",
    "- **update_detector_version**: Updates a detector version. The detector version attributes that you can update include models, external model endpoints, rules, rule execution mode, and description. You can only update a DRAFT detector version\n",
    "- **update_detector_version_status**: Updates the detector version’s status. You can perform the following promotions or demotions using UpdateDetectorVersionStatus: DRAFT to ACTIVE, ACTIVE to INACTIVE, and INACTIVE to ACTIVE\n",
    "- **describe_detector**: Gets all versions for a specified detector\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- put detector, initalizes your detector -- \n",
    "response = client.put_detector(\n",
    "    detectorId    = DETECTOR_NAME, \n",
    "    description   = DETECTOR_DESC,\n",
    "    eventTypeName = EVENT_TYPE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- check the score thresholds with FPR from 1% to 6% --\n",
    "model_stat = df_model.sort_values(by='fpr')\n",
    "model_stat['fpr_bin']=np.ceil(model_stat['fpr']*100)*0.01\n",
    "m = model_stat.loc[model_stat.groupby([\"fpr_bin\"])[\"threshold\"].idxmin()] \n",
    "m = m.round(decimals=2)[['fpr','precision','tpr','threshold']]\n",
    "print (\" --- score thresholds 1% to 6% --- \")\n",
    "print(m.loc[(m['fpr'] > 0.0 ) & (m['fpr'] <= 0.06)].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- decide what threshold and corresponding outcome you want to add -- \n",
    "# here, we create three simple rules by cutting the score at [950,750], and create three outcome ['fraud', 'investigate', 'approve'] \n",
    "# it will create 3 rules:\n",
    "#    score > 950: fraud\n",
    "#    score > 750: investigate \n",
    "#    score <= 750: approve\n",
    "\n",
    "score_cuts = [950,750]                          # recommended to fine tune this based on your business use case\n",
    "outcomes = ['fraud', 'investigate', 'approve']  # recommended to define this based on your business use case\n",
    "\n",
    "def create_outcomes(outcomes):\n",
    "    \"\"\" \n",
    "    Create Fraud Detector Outcomes \n",
    "    \"\"\"   \n",
    "    for outcome in outcomes:\n",
    "        print(\"creating outcome variable: {0} \".format(outcome))\n",
    "        response = client.put_outcome(name = outcome, description = outcome)\n",
    "\n",
    "def create_rules(score_cuts, outcomes):\n",
    "    \"\"\"\n",
    "    Creating rules \n",
    "    \n",
    "    Arguments:\n",
    "        score_cuts  - list of score cuts to create rules\n",
    "        outcomes    - list of outcomes associated with the rules\n",
    "    \n",
    "    Returns:\n",
    "        a rule list to used when create detector\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(score_cuts)+1 != len(outcomes):\n",
    "        logging.error('Your socre cuts and outcomes are not matched.')\n",
    "    \n",
    "    rule_list = []\n",
    "    for i in range(len(outcomes)):\n",
    "        # rule expression\n",
    "        if i < (len(outcomes)-1):\n",
    "            rule = \"${0}_insightscore > {1}\".format(MODEL_NAME,score_cuts[i])\n",
    "        else:\n",
    "            rule = \"${0}_insightscore <= {1}\".format(MODEL_NAME,score_cuts[i-1])\n",
    "    \n",
    "        # append to rule_list (used when create detector)\n",
    "        rule_id = \"rules{0}_{1}\".format(i, MODEL_NAME)\n",
    "        \n",
    "        rule_list.append({\n",
    "            \"ruleId\": rule_id, \n",
    "            \"ruleVersion\" : '1',\n",
    "            \"detectorId\"  : DETECTOR_NAME\n",
    "        })\n",
    "        \n",
    "        # create rules\n",
    "        print(\"creating rule: {0}: IF {1} THEN {2}\".format(rule_id, rule, outcomes[i]))\n",
    "        try:\n",
    "            response = client.create_rule(\n",
    "                ruleId = rule_id,\n",
    "                detectorId = DETECTOR_NAME,\n",
    "                expression = rule,\n",
    "                language = 'DETECTORPL',\n",
    "                outcomes = [outcomes[i]]\n",
    "                )\n",
    "        except:\n",
    "            print(\"this rule already exists in this detector\")\n",
    "            \n",
    "    return rule_list\n",
    "     \n",
    "# -- create outcomes -- \n",
    "print(\" -- create outcomes --\")\n",
    "create_outcomes(outcomes)\n",
    "\n",
    "# -- create rules --\n",
    "print(\" -- create rules --\")\n",
    "rule_list = create_rules(score_cuts, outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- create detector version --\n",
    "client.create_detector_version(\n",
    "    detectorId    = DETECTOR_NAME,\n",
    "    rules         = rule_list,\n",
    "    modelVersions = [{\"modelId\": MODEL_NAME, \n",
    "                      \"modelType\": MODEL_TYPE,\n",
    "                      \"modelVersionNumber\": model_version}],\n",
    "    # there are 2 options for ruleExecutionMode:\n",
    "    #   'ALL_MATCHED'    - return all matched rules' outcome\n",
    "    #   'FIRST_MATCHED'  - return first matched rule's outcome\n",
    "    ruleExecutionMode = 'FIRST_MATCHED'\n",
    ")\n",
    "\n",
    "print(\"\\n -- detector created -- \")\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- activate the latest detector version --\n",
    "detector_version_summaries = client.describe_detector(detectorId=DETECTOR_NAME)['detectorVersionSummaries']\n",
    "latest_detector_version = max([det['detectorVersionId'] for det in detector_version_summaries])\n",
    "print('Latest Detector Version:', latest_detector_version)\n",
    "\n",
    "response = client.update_detector_version_status(\n",
    "    detectorId        = DETECTOR_NAME,\n",
    "    detectorVersionId = latest_detector_version,\n",
    "    status            = 'ACTIVE'\n",
    ")\n",
    "print(\"\\n -- detector activated -- \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make predictions  <a id=make_predictions></a>\n",
    "-----\n",
    "\n",
    "The following section will apply your detector to the first 100 records \n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> API for Making Predictions </strong>\n",
    "\n",
    "- **get_event_prediction**: Evaluates an event against a detector version. If a version ID is not provided, the detector’s (ACTIVE) version is used.  \n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- load test data --\n",
    "s3   = boto3.resource('s3')\n",
    "obj  = s3.Object(S3_BUCKET, S3_FILE_TEST)\n",
    "body = obj.get()['Body']\n",
    "df_test   = pd.read_csv(body, dtype={VARIABLES_MAP['EVENT_LABEL']: object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def _predict(record):\n",
    "    \"\"\"\n",
    "    Get prediction on one event\n",
    "    \"\"\"\n",
    "    event_id = str(uuid.uuid1())\n",
    "    entity_id = str(uuid.uuid1())\n",
    "    event_timestamp = str(record[0])\n",
    "    \n",
    "    try:\n",
    "        rec_content = {event_variables[i]: str(record[2:][i]) for i in range(len(event_variables)) if pd.isnull(record[2+i])==False}\n",
    "        pred = client.get_event_prediction(\n",
    "            detectorId        = DETECTOR_NAME,\n",
    "            detectorVersionId = latest_detector_version,\n",
    "            eventId           = event_id,\n",
    "            eventTypeName     = EVENT_TYPE,\n",
    "            eventTimestamp    = event_timestamp, \n",
    "            entities          = [{\n",
    "                'entityType': ENTITY_TYPE, \n",
    "                'entityId': entity_id\n",
    "            }],\n",
    "            eventVariables    = rec_content) \n",
    "        record.append(pred['modelScores'][0]['scores'][\"{0}_insightscore\".format(MODEL_NAME)])\n",
    "        record.append(pred['ruleResults'][0]['outcomes'])\n",
    "    except:\n",
    "        record.append(\"-999\")\n",
    "        record.append([\"error\"])\n",
    "    \n",
    "    return record\n",
    "\n",
    "\n",
    "# -- get predictions in parallel -- \n",
    "cols_keep = [VARIABLES_MAP['EVENT_TIMESTAMP'], VARIABLES_MAP['EVENT_LABEL']] + event_variables\n",
    "df_list = df_test[cols_keep].values.tolist()\n",
    "with Pool(processes = 5) as p:\n",
    "    result = p.map(_predict, df_list)\n",
    "predictions = pd.DataFrame(result, columns = cols_keep + ['score', 'outcomes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- check the first 5 rows --\n",
    "predictions.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- check the distribution by labels --\n",
    "plt.figure(figsize = (20,8))\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "plt.hist([predictions[predictions[VARIABLES_MAP['EVENT_LABEL']].isin(label_mapper['LEGIT'])]['score'], \n",
    "          predictions[predictions[VARIABLES_MAP['EVENT_LABEL']].isin(label_mapper['FRAUD'])]['score']], bins = 50)\n",
    "plt.legend([\"Legit\", \"Fraud\"], fontsize=12)\n",
    "plt.title(\"Predicted Score Distribution By Label\")\n",
    "plt.xlabel(\"Predicted Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- check AUC --\n",
    "predictions['event_label_int'] = np.nan\n",
    "predictions.loc[predictions[VARIABLES_MAP['EVENT_LABEL']].isin(label_mapper['LEGIT']), 'event_label_int'] = 0\n",
    "predictions.loc[predictions[VARIABLES_MAP['EVENT_LABEL']].isin(label_mapper['FRAUD']), 'event_label_int'] = 1\n",
    "                \n",
    "fpr, tpr, threshold = roc_curve(predictions['event_label_int'], predictions['score'])\n",
    "test_auc = auc(fpr,tpr)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"AUC: {test_auc:.2f}\") \n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.title(MODEL_NAME+\" ROC Curve (Test)\")\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (FPR)')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Write Predictions to File\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡<strong> Write Predictions </strong>\n",
    "\n",
    "- You can write your prediction dataset to a CSV to manually review predictions\n",
    "- Simply add a cell below and copy the code below\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# -- optionally write predictions to a CSV file -- \n",
    "predictions.to_csv(MODEL_NAME + \".csv\", index=False)\n",
    "# -- or to a XLS file \n",
    "predictions.to_excel(MODEL_NAME + \".xlsx\", index=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #restore original boto3 version\n",
    "# %pip install 'boto3=={}'.format(original_boto3_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
