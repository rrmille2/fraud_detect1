{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud Detector\n",
    "#### Data Prep Notebook\n",
    "\n",
    "This notebook makes use of a data set which can be found here:  \n",
    "https://github.com/mwitiderrick/insurancedata/blob/master/insurance_claims.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3bucket = 'your-bucket-name'\n",
    "s3prefix = 'your-prefix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/insurance_claims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide an 'Unknown' category for any cells with a question mark\n",
    "df1.replace('?', 'Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the label must be lower case\n",
    "df1['fraud_reported'].replace({'Y':'y', 'N': 'n'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand our data set from 1,000 rows to 12,000 rows\n",
    "This is done in several steps (in the next two cells):\n",
    "1. Get lists of unique values from several columns\n",
    "2. Randomize some columns of data in a second dataframe\n",
    "3. Append this randomized dataframe (df2) to the original dataframe\n",
    "4. Repeat multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [ 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA',\n",
    "           'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
    "           'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM',\n",
    "           'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
    "           'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_makes = df1.auto_make.unique()\n",
    "auto_models = df1.auto_model.unique()\n",
    "auto_years = df1.auto_year.unique()\n",
    "incident_cities = df1.incident_city.unique()\n",
    "#incident_states = df.incident_state.unique()\n",
    "incident_states = states\n",
    "incident_locations = df1.incident_location.unique()\n",
    "insured_zips = df1.insured_zip.unique()\n",
    "insured_education_levels = df1.insured_education_level.unique()\n",
    "collision_types = df1.collision_type.unique()\n",
    "police_report_availables = df1.police_report_available.unique()\n",
    "policy_numbers = df1.policy_number.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.copy()\n",
    "\n",
    "for i in range(0,12):\n",
    "    df2 = df1.copy()\n",
    "    df2.incident_state = np.random.choice(incident_states)\n",
    "    df2.incident_city = np.random.choice(incident_cities)\n",
    "    df2.insured_education_level = np.random.choice(insured_education_levels)\n",
    "    df2.collision_type = np.random.choice(collision_types)\n",
    "    df2.incident_location = np.random.choice(incident_locations)\n",
    "    df2.police_report_available = np.random.choice(police_report_availables)\n",
    "    df2.policy_number = np.random.choice(policy_numbers)\n",
    "    df = pd.concat([df, df2], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this columns appears to be empty\n",
    "df.drop(['_c39'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't have hyphens in the column header names\n",
    "df.rename(columns={'capital-gains': 'capital_gains', 'capital-loss': 'capital_loss'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine incident date and hour to a single date/time field\n",
    "#df['baz'] = df.agg(lambda x: f\"{x['bar']} is {x['foo']}\", axis=1)\n",
    "df['EVENT_TIMESTAMP'] = df.agg( lambda x: f'{x[\"incident_date\"]}T{x[\"incident_hour_of_the_day\"]:02d}:00:00Z', axis=1 )\n",
    "df.drop(columns={'incident_date', 'incident_hour_of_the_day'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'fraud_reported': 'EVENT_LABEL', 'policy_number': 'ENTITY_ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.countplot(x='EVENT_LABEL',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set into train and test\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframes to csv files\n",
    "train.to_csv('data/train.csv', index=None)\n",
    "test.to_csv('data/test.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "!aws s3 cp data/train.csv s3://$s3bucket/$s3prefix/train/\n",
    "!aws s3 cp data/test.csv s3://$s3bucket/$s3prefix/test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
